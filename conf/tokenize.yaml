# Default tokenization configuration
# Override specific settings with command line: python scripts/tokenize_csv.py input.csv_file=data/sample.csv

input:
  # Input source configuration - specify ONE of these
  csv_file: null                    # Path to CSV file
  json_folder: null                 # Path to JSON folder (enables HTML processing)
  dataframe_file: null              # Path to DataFrame CSV for merging (requires json_folder)
  
processing:
  # Processing configuration
  batch_size: 500                   # Records per batch
  max_content_length: 10000         # Maximum HTML content length
  extra_columns:                    # Specific DataFrame columns to merge (list)
    - cust_status                   # Example: Customer status
    - revenue                       # Example: Company revenue
    - market_segment                # Example: Market segment
    # - sales_rep                   # Example: Sales representative (commented out)
  use_multiprocessing: true         # Enable multiprocessing for tokenization
  num_processes: null               # Number of processes (null = auto-detect CPU cores)
  use_hybrid_pipeline: true        # Enable hybrid async I/O + multiprocessing pipeline
  max_concurrent_io: 20             # Maximum concurrent I/O operations
  
output:
  # Output configuration
  output_dir: null                  # Output directory (auto-generated if null)
  clear_output: false               # Clear output directory before processing
  
tokenizer:
  # Tokenizer settings
  included_pos: 
    - "名詞"                        # Nouns
    - "動詞"                        # Verbs  
    - "形容詞"                      # Adjectives
    - "副詞"                        # Adverbs
  min_word_length: 2                # Minimum word length
  stop_words_filtered: true         # Filter stop words