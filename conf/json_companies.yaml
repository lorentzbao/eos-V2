# Preset: JSON Companies Processing
# Optimized for processing JSON company data with HTML content and DataFrame merging
# Usage: uv run python scripts/tokenize_csv.py --config-path conf/presets --config-name json_companies

input:
  csv_file: null
  json_folder: "data/raw/tokyo"  # Default to tokyo, can be overridden
  dataframe_file: "data/sample_companies.csv"

  # Root path configuration for HTML files
  primary_root_path: ""             # Primary root path to prepend to HTML file paths (e.g., "/mnt/e/data")
  secondary_root_path: ""           # Secondary root path to try if primary fails (e.g., "/home/user/data")

processing:
  batch_size: 256
  max_content_length: 5000
  extra_columns:
    - CUST_STATUS2
    - LARGE_CLASS_NAME
    - MIDDLE_CLASS_NAME
    - CURR_SETLMNT_TAKING_AMT
    - EMPLOYEE_ALL_NUM
  use_multiprocessing: true
  num_processes: null               # Auto-detect CPU cores
  use_hybrid_pipeline: true        # Enable hybrid async I/O + multiprocessing pipeline
  max_concurrent_io: 8             # Maximum concurrent I/O operations
  max_subdomains_per_company: 100   # Limit sub-domains per company (null = no limit)

output:
  output_dir: null                  # Auto-generate: data/tokenized/{prefecture}
  clear_output: false

tokenizer:
  type: janome                      # Tokenizer backend: 'janome', 'mecab', or null for auto-detect
  included_pos:
    - "名詞"
    - "動詞"
    - "形容詞"
    - "副詞"
  min_word_length: 2
  stop_words_filtered: true