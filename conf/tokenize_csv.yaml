# Configuration for traditional CSV tokenization
input:
  csv_file: "data/sample_companies.csv"
  json_folder: null
  dataframe_file: null
  
processing:
  batch_size: 1000
  max_content_length: 10000
  extra_columns: null               # Not applicable for CSV input
  
output:
  output_dir: null                  # Auto-generate: data/sample_companies/tokenized
  clear_output: true                # Clear existing output
  
tokenizer:
  included_pos: 
    - "名詞"
    - "動詞"
    - "形容詞"
    - "副詞"
  min_word_length: 2
  stop_words_filtered: true