# EOS - Japanese Enterprise Search Engine

Modern Flask-based search engine for Japanese companies with intelligent grouping and advanced text processing.

---

## ğŸš€ Quick Start

```bash
# Install and run
uv run python run.py

# Open browser
http://127.0.0.1:5000
```

**Default Login:** Enter any username to start

---

## ğŸ“‹ Services Overview

### **Search Services**
- **Company Search** - Intelligent grouping of multiple URLs per company
- **Japanese Text Processing** - Janome tokenization with normalization  
- **Auto-suggestions** - Google-style dropdown with popular terms
- **Prefecture Filtering** - Location-based company filtering

### **Data Export Services**  
- **CSV Export** - Download search results with company structure
- **File Caching** - Performance optimization for repeated exports
- **UTF-8 BOM Encoding** - Excel-compatible Japanese text

### **User Services**
- **Authentication** - Simple username-based sessions
- **Search History** - Track past searches with query normalization
- **Rankings** - Real-time popular keyword tracking
- **Pagination** - Client-side navigation through company cards

### **Data Management Services**
- **Index Management** - Add/clear/optimize search index
- **Batch Loading** - CSV-based bulk data import
- **LRU Cache** - Built-in result caching (128 entries)
- **Query Logging** - User search tracking with normalization

---

## ğŸ” How to Use Each Service

### 1. **Company Search**
```bash
# Access via browser
http://127.0.0.1:5000

# Search Japanese companies
Query: "Python é–‹ç™º"
Query: "AI äººå·¥çŸ¥èƒ½" 
Query: "ç ”ç©¶ã€€é–‹ç™º"  # Normalized automatically
```

**Features:**
- Multiple URLs grouped by company
- Match highlighting for each URL
- Prefecture filtering (Tokyo, Osaka, etc.)
- 10/20/50 results per page

### 2. **CSV Export**
```bash
# Download search results
http://127.0.0.1:5000/api/download-csv?q=Python&prefecture=tokyo
```

**Output Format:**
```csv
Company_Number,Company_Name,Company_Tel,Company_Industry,Prefecture,URL_Name,URL,Content,Matched_Terms,ID
1010001000001,æ ªå¼ä¼šç¤¾æ±äº¬ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼,03-1234-5678,æƒ…å ±é€šä¿¡æ¥­,tokyo,ãƒ¡ã‚¤ãƒ³ã‚µã‚¤ãƒˆ,https://tokyo-tech.co.jp,æ±äº¬ã‚’æ‹ ç‚¹ã¨ã™ã‚‹...,python,url_001
```

### 3. **Search History**
```bash
# View user search logs
http://127.0.0.1:5000/history

# Show all searches
http://127.0.0.1:5000/history?show_all=true
```

**Features:**
- Normalized query consistency ("ç ”ç©¶ã€€é–‹ç™º" = "ç ”ç©¶ é–‹ç™º")
- Timestamp, search type, results count
- Prefecture filter tracking
- Scalable pagination (8 recent, up to 100 total)

### 4. **Popular Rankings**
```bash
# View trending keywords  
http://127.0.0.1:5000/rankings
```

**Features:**
- Real-time keyword popularity
- Search count and percentage
- Japanese text normalization applied
- Medal badges for top terms

### 5. **Data Management**
```bash
# Add single document
POST /api/add_document
{
  "id": "url_001",
  "company_name": "æ ªå¼ä¼šç¤¾ãƒ†ã‚¹ãƒˆ",
  "company_number": "1010001000001",
  "content": "ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„"
}

# Clear index
POST /api/clear_index

# Optimize index  
POST /api/optimize_index
```

### 6. **Data Import**
```bash
# Load sample data
uv run python load_sample_data.py

# Or via API for individual records
POST /api/add_document     # Single record
POST /api/add_documents    # Batch records
```

**Enterprise CSV Format:**
```csv
id,jcn,cust_status,company_name_kj,company_address_all,duns_large_class_name,duns_middle_class_name,curr_setlmnt_taking_amt,employee,prefecture,city,district_finalized_cd,branch_name_cd,main_domain_url,url_name,url,content,title
url_001,1010001000001,å„ªè‰¯é¡§å®¢,æ ªå¼ä¼šç¤¾æ±äº¬ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼,æ±äº¬éƒ½æ¸¯åŒºè™ãƒé–€1-1-1,æƒ…å ±é€šä¿¡æ¥­,ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢æ¥­,500000000,250,tokyo,æ¸¯åŒº,TK001,BR001,https://tokyo-tech.co.jp,ãƒ¡ã‚¤ãƒ³ã‚µã‚¤ãƒˆ,https://tokyo-tech.co.jp/index.html,æ±äº¬ã‚’æ‹ ç‚¹ã¨ã™ã‚‹ITä¼æ¥­ã§ã™...,æ ªå¼ä¼šç¤¾æ±äº¬ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ - ãƒ¡ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
```

**Enterprise Field Requirements:**
- `id` - Unique URL identifier
- `jcn` - æ³•äººç•ªå· (Corporate Number, grouping key)
- `cust_status` - é¡§å®¢åŒºåˆ† (Customer status)
- `company_name_kj` - æ¼¢å­—å (Company name in Kanji)
- `company_address_all` - ä½æ‰€ (Full address)
- `duns_large_class_name` - æ¥­ç¨®å¤§åˆ†é¡ (Major industry classification)
- `duns_middle_class_name` - æ¥­ç¨®ä¸­åˆ†é¡ (Minor industry classification)
- `curr_setlmnt_taking_amt` - å£²ä¸Šé«˜ (Revenue)
- `employee` - å¾“æ¥­å“¡æ•° (Employee count)
- `prefecture` - éƒ½é“åºœçœŒ (Prefecture code)
- `city` - å¸‚åŒºç”ºæ‘ (City/Ward)
- `district_finalized_cd` - äº‹æ¥­æœ¬éƒ¨ã‚³ãƒ¼ãƒ‰ (District code)
- `branch_name_cd` - æ”¯åº—ã‚³ãƒ¼ãƒ‰ (Branch code)
- `main_domain_url` - ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸URL (Main website URL)
- `url_name` - URL description (page type)
- `url` - å½“è©²url (Specific page URL)
- `content` - Searchable text content
- `title` - Display title

**Loading Script:**
```python
# Load your own CSV data
import csv
from app.services.search_service import SearchService

def load_csv_data(csv_file_path):
    search_service = SearchService()
    with open(csv_file_path, 'r', encoding='utf-8') as f:
        companies = list(csv.DictReader(f))
    
    search_service.clear_index()
    success = search_service.add_documents_batch(companies)
    print(f"Loaded {len(companies)} records: {success}")

load_csv_data('company_data.csv')
```

**Import tips:**
- Use UTF-8 encoding for Japanese text
- Multiple URLs per company share same `company_number`
- Ready to use with included sample data (100+ records, 47 companies)

---

## ğŸ“Š Sample Usage

### **Search Examples**
```bash
# Technology companies
"Python" â†’ Python development companies
"AI" â†’ Artificial intelligence companies  
"ãƒ•ã‚£ãƒ³ãƒ†ãƒƒã‚¯" â†’ Fintech companies
"ã‚²ãƒ¼ãƒ " â†’ Gaming companies

# With filters
"æ©Ÿæ¢°å­¦ç¿’" + prefecture:"tokyo" â†’ Tokyo ML companies
"ç ”ç©¶ é–‹ç™º" â†’ R&D companies (normalized text)
```

### **CSV Export Examples**
```bash
# All Python companies
/api/download-csv?q=Python

# Tokyo fintech companies
/api/download-csv?q=ãƒ•ã‚£ãƒ³ãƒ†ãƒƒã‚¯&prefecture=tokyo

# Title-only search export
/api/download-csv?q=AI&type=title
```

---

## ğŸ› ï¸ Technical Architecture

### **Backend Services**
- **Flask** - Web framework with session management
- **Whoosh** - Full-text search engine with company grouping
- **Janome** - Japanese tokenization and text processing
- **LRU Cache** - Built-in result caching (auto-clearing)

### **Frontend Services**  
- **Bootstrap 5** - Responsive UI framework
- **JavaScript** - Client-side pagination and suggestions
- **Company Cards** - Clean visual hierarchy for grouped results
- **Auto-suggestions** - Google-style dropdown with keyboard navigation

### **Data Services**
- **JSON Records** - Company and URL metadata storage
- **File-based Caching** - CSV export performance optimization  
- **Query Normalization** - Japanese text consistency across services
- **Session Logging** - Per-user search tracking

---

## ğŸ“š Service Documentation

**For Developers:** [Complete API Reference](./FRONTEND_API_DOCS.md)

**Quick API Reference:**
- `GET /search` - HTML search with company grouping
- `GET /api/search` - JSON search API  
- `GET /api/download-csv` - CSV export (authenticated)
- `POST /api/add_document` - Add single record
- `POST /api/add_documents` - Batch add records
- `GET /history` - Search history page
- `GET /rankings` - Popular keywords page

---

## ğŸ”§ Development Setup

```bash
# Install dependencies
pip install Flask==3.0.0 Janome==0.5.0 Whoosh==2.7.4

# Run with auto-reload
uv run python run.py

# Load sample data (100+ records)
uv run python load_sample_data.py

# Access services
http://127.0.0.1:5000
```

